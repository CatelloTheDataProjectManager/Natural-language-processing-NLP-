{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6870ae4d-22c2-4978-adb7-f6cca6e54384",
   "metadata": {},
   "source": [
    "# TP : Système de Recommandation avec un LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f09558-490c-40ea-92a1-cac30fe0c357",
   "metadata": {},
   "source": [
    "### Etape 1 : Importation des données\n",
    "\n",
    "#### 1. Téléchargement des données :\n",
    "- Téléchargez le dataset \"E-Commerce Dataset\" depuis Kaggle en utilisant l’API kagglehub.\n",
    "- Lien vers le dataset : [E-Commerce Dataset](https://www.kaggle.com/datasets/davidafolayan/e-commerce-dataset/data)\n",
    "\n",
    "### Etape 2 : Chargement et exploration des données\n",
    "\n",
    "#### 2. Charger les données :\n",
    "- Chargez le fichier CSV contenant les données transactionnelles en utilisant pandas.\n",
    "- Affichez un aperçu des colonnes disponibles et des premières lignes du dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75593d47-7cab-4c05-8d41-597af4d19c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_first_name</th>\n",
       "      <th>customer_last_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>product_name</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>customer_country</th>\n",
       "      <th>customer_region</th>\n",
       "      <th>...</th>\n",
       "      <th>order_date</th>\n",
       "      <th>order_id</th>\n",
       "      <th>ship_date</th>\n",
       "      <th>shipping_type</th>\n",
       "      <th>days_for_shipment_scheduled</th>\n",
       "      <th>days_for_shipment_real</th>\n",
       "      <th>order_item_discount</th>\n",
       "      <th>sales_per_order</th>\n",
       "      <th>order_quantity</th>\n",
       "      <th>profit_per_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_45866</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Fuller</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Xerox 1913</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>New Rochelle</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>East</td>\n",
       "      <td>...</td>\n",
       "      <td>11/5/2022</td>\n",
       "      <td>O_ID_3001072</td>\n",
       "      <td>11/7/2022</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>35.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5</td>\n",
       "      <td>223.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_44932</td>\n",
       "      <td>Alan</td>\n",
       "      <td>Edelman</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>#6 3/4 Gummed Flap White Envelopes</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Texas</td>\n",
       "      <td>United States</td>\n",
       "      <td>Central</td>\n",
       "      <td>...</td>\n",
       "      <td>20-06-2022</td>\n",
       "      <td>O_ID_3009170</td>\n",
       "      <td>23-06-2022</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5</td>\n",
       "      <td>199.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_70880</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Gayman</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Belkin 8 Outlet Surge Protector</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Louisville</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>United States</td>\n",
       "      <td>South</td>\n",
       "      <td>...</td>\n",
       "      <td>25-06-2022</td>\n",
       "      <td>O_ID_3047567</td>\n",
       "      <td>30-06-2022</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>75.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5</td>\n",
       "      <td>195.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_33157</td>\n",
       "      <td>Raymond</td>\n",
       "      <td>Eason</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>GBC VeloBinder Manual Binding System</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>United States</td>\n",
       "      <td>Central</td>\n",
       "      <td>...</td>\n",
       "      <td>10/6/2022</td>\n",
       "      <td>O_ID_3060575</td>\n",
       "      <td>10/10/2022</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>1</td>\n",
       "      <td>220.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_58303</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Gonzalez</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Eldon Pizzaz Desk Accessories</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>United States</td>\n",
       "      <td>East</td>\n",
       "      <td>...</td>\n",
       "      <td>2/5/2022</td>\n",
       "      <td>O_ID_3064311</td>\n",
       "      <td>8/1/2022</td>\n",
       "      <td>First Class</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>125.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>97.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id customer_first_name customer_last_name    category_name  \\\n",
       "0  C_ID_45866                Mary             Fuller  Office Supplies   \n",
       "1  C_ID_44932                Alan            Edelman  Office Supplies   \n",
       "2  C_ID_70880                Mary             Gayman  Office Supplies   \n",
       "3  C_ID_33157             Raymond              Eason  Office Supplies   \n",
       "4  C_ID_58303                Mary           Gonzalez        Furniture   \n",
       "\n",
       "                           product_name customer_segment customer_city  \\\n",
       "0                            Xerox 1913        Corporate  New Rochelle   \n",
       "1    #6 3/4 Gummed Flap White Envelopes        Corporate       Houston   \n",
       "2       Belkin 8 Outlet Surge Protector         Consumer    Louisville   \n",
       "3  GBC VeloBinder Manual Binding System        Corporate       Chicago   \n",
       "4         Eldon Pizzaz Desk Accessories      Home Office  Philadelphia   \n",
       "\n",
       "  customer_state customer_country customer_region  ...  order_date  \\\n",
       "0       New York    United States            East  ...   11/5/2022   \n",
       "1          Texas    United States         Central  ...  20-06-2022   \n",
       "2       Kentucky    United States           South  ...  25-06-2022   \n",
       "3       Illinois    United States         Central  ...   10/6/2022   \n",
       "4   Pennsylvania    United States            East  ...    2/5/2022   \n",
       "\n",
       "       order_id   ship_date   shipping_type days_for_shipment_scheduled  \\\n",
       "0  O_ID_3001072   11/7/2022    Second Class                           2   \n",
       "1  O_ID_3009170  23-06-2022    Second Class                           2   \n",
       "2  O_ID_3047567  30-06-2022  Standard Class                           4   \n",
       "3  O_ID_3060575  10/10/2022    Second Class                           2   \n",
       "4  O_ID_3064311    8/1/2022     First Class                           1   \n",
       "\n",
       "   days_for_shipment_real  order_item_discount  sales_per_order  \\\n",
       "0                       2                 35.0            500.0   \n",
       "1                       3                 85.0            500.0   \n",
       "2                       5                 75.0             44.0   \n",
       "3                       4                 60.0            254.0   \n",
       "4                       2                125.0            500.0   \n",
       "\n",
       "   order_quantity  profit_per_order  \n",
       "0               5        223.199997  \n",
       "1               5        199.199997  \n",
       "2               5        195.500000  \n",
       "3               1        220.000000  \n",
       "4               1         97.500000  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les données dans un DataFrame pandas\n",
    "df = pd.read_csv(r\"C:\\Users\\cated\\Desktop\\dev\\IA school\\Machine learning supervisé recommandation\\Ecommerce_data.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e727c21-6c4e-47e3-8cfb-220fbb875629",
   "metadata": {},
   "source": [
    "### Etape 3 : Prétaitement des données\n",
    "\n",
    "#### 3. Sélectionner les colonnes pertinentes :\n",
    "- Gardez uniquement les colonnes suivantes : customer id, product name, et order date.\n",
    "\n",
    "#### 4. Réduction de la taille du dataset :\n",
    "- Réduisez la taille du dataset à 20 % pour simplifier l’exécution des modèles.\n",
    "\n",
    "#### 5. Conversion des dates et tri :\n",
    "- Convertissez la colonne order date en format datetime et triez les achats par customer id et order date.\n",
    "\n",
    "#### 6. Agrégation des séquences :\n",
    "- Regroupez les achats par client en créant une séquence unique de produits achetés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b3c8bf8-1a15-4901-a3b1-2f558ddbbfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame réduit :\n",
      "  customer_id                          product_name  order_date\n",
      "0  C_ID_45866                            Xerox 1913   11/5/2022\n",
      "1  C_ID_44932    #6 3/4 Gummed Flap White Envelopes  20-06-2022\n",
      "2  C_ID_70880       Belkin 8 Outlet Surge Protector  25-06-2022\n",
      "3  C_ID_33157  GBC VeloBinder Manual Binding System   10/6/2022\n",
      "4  C_ID_58303         Eldon Pizzaz Desk Accessories    2/5/2022\n",
      "Nombre de lignes après réduction : 22654\n",
      "       customer_id                                       product_name  \\\n",
      "112094  C_ID_25005                             GBC Wire Binding Combs   \n",
      "43163   C_ID_25006                     Cisco SPA525G2 5-Line IP Phone   \n",
      "43165   C_ID_25006  Kensington 7 Outlet MasterPiece Power Center w...   \n",
      "18488   C_ID_25008                                         Newell 318   \n",
      "29860   C_ID_25011      Wilson Jones 1\" Hanging DublLock Ring Binders   \n",
      "\n",
      "       order_date  \n",
      "112094 2022-08-24  \n",
      "43163  2021-07-24  \n",
      "43165  2021-08-20  \n",
      "18488  2022-02-15  \n",
      "29860  2022-10-22  \n",
      "  customer_id                                  products_sequence\n",
      "0  C_ID_25005                             GBC Wire Binding Combs\n",
      "1  C_ID_25006  Cisco SPA525G2 5-Line IP Phone, Kensington 7 O...\n",
      "2  C_ID_25008                                         Newell 318\n",
      "3  C_ID_25011      Wilson Jones 1\" Hanging DublLock Ring Binders\n",
      "4  C_ID_25012  Acme Office Executive Series Stainless Steel T...\n"
     ]
    }
   ],
   "source": [
    "# Étape 3 : Prétaitement des données\n",
    "# Sélectionner les colonnes pertinentes\n",
    "df_reduit = df[['customer_id', 'product_name', 'order_date']]\n",
    "\n",
    "# Afficher les premières lignes du DataFrame réduit\n",
    "print(\"\\nDataFrame réduit :\")\n",
    "print(df_reduit.head())\n",
    "\n",
    "# Réduction de la taille du dataset à 20%\n",
    "df_reduit = df_reduit.sample(frac=0.20, random_state=1)\n",
    "print(f\"Nombre de lignes après réduction : {len(df_reduit)}\")\n",
    "\n",
    "# Conversion des dates en format datetime en gérant les différents formats\n",
    "df_reduit['order_date'] = pd.to_datetime(df_reduit['order_date'], format=\"%d-%m-%Y\", errors='coerce')\n",
    "df_reduit['order_date'] = df_reduit['order_date'].fillna(pd.to_datetime(df_reduit['order_date'], format=\"%m/%d/%Y\", errors='coerce'))\n",
    "\n",
    "# Enlever les dates invalides\n",
    "df_reduit = df_reduit.dropna(subset=['order_date'])\n",
    "\n",
    "# Trier les achats par customer_id et order_date\n",
    "df_reduit = df_reduit.sort_values(by=['customer_id', 'order_date'])\n",
    "print(df_reduit.head())\n",
    "\n",
    "# Agrégation des séquences\n",
    "df_agg = df_reduit.groupby('customer_id')['product_name'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "df_agg.rename(columns={'product_name': 'products_sequence'}, inplace=True)\n",
    "print(df_agg.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d525d-e0ad-4818-a4fb-f3df392aea10",
   "metadata": {},
   "source": [
    "### Etape 4 : Tokenisation et création des paires d’entrée-sortie\n",
    "\n",
    "#### 7. Tokenisation des séquences :\n",
    "- Utilisez un tokenizer pour convertir les noms de produits en indices numériques.\n",
    "\n",
    "#### 8. Créer les paires entrée-sortie :\n",
    "- Créez des paires X et y où X est une séquence partielle et y est le produit suivant dans la séquence.\n",
    "\n",
    "#### 9. Remplir les séquences :\n",
    "- Utilisez le padding pour uniformiser les longueurs des séquences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a8021489-63ba-45d4-8d39-320a4110730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# 7. Tokenisation des séquences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_agg['products_sequence'])\n",
    "sequences = tokenizer.texts_to_sequences(df_agg['products_sequence'])\n",
    "\n",
    "# 8. Créer les paires entrée-sortie\n",
    "X, y = [], []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        X.append(seq[:i])\n",
    "        y.append(seq[i])\n",
    "\n",
    "# Trouver la longueur maximale des séquences\n",
    "max_sequence_length = max(len(seq) for seq in X)\n",
    "\n",
    "# 9. Remplir les séquences\n",
    "X = pad_sequences(X, maxlen=max_sequence_length, padding='pre')\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931a437-da3c-48cb-9595-930330fcf2ee",
   "metadata": {},
   "source": [
    "### Etape 5 : Création et entraînement du modèle LSTM\n",
    "\n",
    "#### 10. Définir le modèle :\n",
    "- Construisez un modèle LSTM avec les couches suivantes :\n",
    "  - Une couche Embedding pour convertir les indices des produits en vecteurs de caractéristiques.\n",
    "  - Une couche LSTM.\n",
    "  - Deux couches Dense, dont la dernière avec une activation softmax.\n",
    "\n",
    "### 11. Compiler et entraîner le modèle :\n",
    "- Compilez le modèle avec une fonction de perte adaptée et entraînez-le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e1652a2-cbc1-4cc1-bf39-4c1c377e325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cated\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - accuracy: 6.2500e-04 - loss: 5.9992 - val_accuracy: 0.0099 - val_loss: 6.0025\n",
      "Epoch 2/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0072 - loss: 5.9895 - val_accuracy: 0.0099 - val_loss: 6.0208\n",
      "Epoch 3/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0132 - loss: 5.9410 - val_accuracy: 0.0000e+00 - val_loss: 6.2588\n",
      "Epoch 4/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0135 - loss: 5.7908 - val_accuracy: 0.0000e+00 - val_loss: 6.5318\n",
      "Epoch 5/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.0122 - loss: 5.6875 - val_accuracy: 0.0000e+00 - val_loss: 6.7070\n",
      "Epoch 6/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.0174 - loss: 5.6641 - val_accuracy: 0.0000e+00 - val_loss: 6.8708\n",
      "Epoch 7/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.0097 - loss: 5.6022 - val_accuracy: 0.0000e+00 - val_loss: 7.0638\n",
      "Epoch 8/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0093 - loss: 5.5744 - val_accuracy: 0.0000e+00 - val_loss: 7.1794\n",
      "Epoch 9/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.0156 - loss: 5.5758 - val_accuracy: 0.0000e+00 - val_loss: 7.2941\n",
      "Epoch 10/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0132 - loss: 5.5620 - val_accuracy: 0.0000e+00 - val_loss: 7.3603\n",
      "Epoch 11/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0221 - loss: 5.5323 - val_accuracy: 0.0000e+00 - val_loss: 7.4189\n",
      "Epoch 12/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0124 - loss: 5.5529 - val_accuracy: 0.0000e+00 - val_loss: 7.4319\n",
      "Epoch 13/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0136 - loss: 5.5754 - val_accuracy: 0.0000e+00 - val_loss: 7.5531\n",
      "Epoch 14/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.0166 - loss: 5.5130 - val_accuracy: 0.0099 - val_loss: 7.7684\n",
      "Epoch 15/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0236 - loss: 5.5097 - val_accuracy: 0.0099 - val_loss: 7.7681\n",
      "Epoch 16/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0082 - loss: 5.5227 - val_accuracy: 0.0099 - val_loss: 7.6152\n",
      "Epoch 17/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0169 - loss: 5.5089 - val_accuracy: 0.0099 - val_loss: 7.6272\n",
      "Epoch 18/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0243 - loss: 5.4558 - val_accuracy: 0.0099 - val_loss: 7.7480\n",
      "Epoch 19/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.0171 - loss: 5.4166 - val_accuracy: 0.0099 - val_loss: 7.7867\n",
      "Epoch 20/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.0228 - loss: 5.4295 - val_accuracy: 0.0099 - val_loss: 7.8078\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Définir les paramètres du modèle\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Taille du vocabulaire\n",
    "embedding_dim = 50  # Dimension des embeddings\n",
    "max_sequence_length = max([len(x) for x in X])  # Longueur maximale des séquences\n",
    "\n",
    "# 10. Définir le modèle\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# 11. Compiler et entraîner le modèle\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59853f8-f83e-4d7d-bc35-337551541c1f",
   "metadata": {},
   "source": [
    "### Etape 6 : Prédiction\n",
    "\n",
    "#### 12. Créer une fonction de prédiction :\n",
    "- Implémentez une fonction qui prend une séquence de produits et retourne les prochains produits les plus probables.\n",
    "\n",
    "#### 13. Tester les prédictions :\n",
    "- Utilisez un exemple de séquence pour générer des prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72040bec-e672-45ba-8f78-929f01866628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Créer une fonction de prédiction\n",
    "def predict_next_products(model, tokenizer, input_sequence, num_predictions=1):\n",
    "    \"\"\"\n",
    "    Prédire les prochains produits les plus probables.\n",
    "\n",
    "    Args:\n",
    "    model -- le modèle LSTM entraîné\n",
    "    tokenizer -- le tokenizer utilisé pour convertir les noms de produits\n",
    "    input_sequence -- la séquence d'entrée des produits\n",
    "    num_predictions -- le nombre de prédictions à générer\n",
    "\n",
    "    Retourne:\n",
    "    Une liste des prochains produits les plus probables\n",
    "    \"\"\"\n",
    "    for _ in range(num_predictions):\n",
    "        # Convertir la séquence d'entrée en indices\n",
    "        encoded_sequence = tokenizer.texts_to_sequences([input_sequence])[0]\n",
    "        # Ajouter un padding pour correspondre à la longueur maximale de séquence\n",
    "        padded_sequence = pad_sequences([encoded_sequence], maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "        # Prédire le produit suivant\n",
    "        predicted_index = model.predict(padded_sequence, verbose=0)\n",
    "        predicted_index = np.argmax(predicted_index, axis=1)[0]\n",
    "\n",
    "        # Convertir l'indice prédit en produit\n",
    "        predicted_product = tokenizer.index_word[predicted_index]\n",
    "        \n",
    "        # Ajouter le produit prédit à la séquence d'entrée\n",
    "        input_sequence += ' ' + predicted_product\n",
    "\n",
    "    return input_sequence.split()[-num_predictions:]\n",
    "\n",
    "# 13. Tester les prédictions\n",
    "example_sequence = \"product_1 product_2 product_3\"\n",
    "predicted_products = predict_next_products(model, tokenizer, example_sequence, num_predictions=3)\n",
    "print(f\"Produits prédits : {predicted_products}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57320c-a39c-4e12-89d3-d82e8a81d31d",
   "metadata": {},
   "source": [
    "### Etape 7 : Questions de réflexion\n",
    "\n",
    "#### 14. Améliorations possibles :\n",
    "- Quelles approches pourriez-vous utiliser pour améliorer les performances du modèle ?\n",
    "  - Augmenter la taille du dataset pour capturer plus de variations.\n",
    "  - Utiliser des techniques de régularisation telles que la L2 regularization ou le dropout pour éviter le surapprentissage.\n",
    "  - Expérimenter avec des architectures de modèles différentes, comme les GRU ou Transformer.\n",
    "  - Fine-tuning des hyperparamètres tels que le taux d'apprentissage, la taille des couches, et le nombre de neurones.\n",
    "\n",
    "- Proposez des métriques d’évaluation adaptées pour ce type de système de recommandation.\n",
    "  - Précision@k : Pourcentage de fois où le produit correct est parmi les k premiers produits recommandés.\n",
    "  - Recall@k : Pourcentage de fois où tous les produits corrects sont parmi les k premiers produits recommandés.\n",
    "  - F1-Score : Harmonie entre la précision et le rappel pour évaluer le modèle.\n",
    "  - AUC-ROC : Mesure la performance du modèle de classification à différents seuils de classification.\n",
    "  - NDCG (Normalized Discounted Cumulative Gain) : Mesure l'efficacité des recommandations en prenant en compte la position des produits recommandés.\n",
    "\n",
    "Essayez ce code pour implémenter les prédictions et utilisez les questions de réflexion pour améliorer et évaluer votre modèle. Si vous avez besoin d'aide supplémentaire ou d'autres modifications, n'hésitez pas à demander! 😊\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d298f905-95f4-4135-8c2b-7b854a808354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu des colonnes :\n",
      "Index(['customer_id', 'customer_first_name', 'customer_last_name',\n",
      "       'category_name', 'product_name', 'customer_segment', 'customer_city',\n",
      "       'customer_state', 'customer_country', 'customer_region',\n",
      "       'delivery_status', 'order_date', 'order_id', 'ship_date',\n",
      "       'shipping_type', 'days_for_shipment_scheduled',\n",
      "       'days_for_shipment_real', 'order_item_discount', 'sales_per_order',\n",
      "       'order_quantity', 'profit_per_order'],\n",
      "      dtype='object')\n",
      "\n",
      "Premières lignes du dataset :\n",
      "  customer_id customer_first_name customer_last_name    category_name  \\\n",
      "0  C_ID_45866                Mary             Fuller  Office Supplies   \n",
      "1  C_ID_44932                Alan            Edelman  Office Supplies   \n",
      "2  C_ID_70880                Mary             Gayman  Office Supplies   \n",
      "3  C_ID_33157             Raymond              Eason  Office Supplies   \n",
      "4  C_ID_58303                Mary           Gonzalez        Furniture   \n",
      "\n",
      "                           product_name customer_segment customer_city  \\\n",
      "0                            Xerox 1913        Corporate  New Rochelle   \n",
      "1    #6 3/4 Gummed Flap White Envelopes        Corporate       Houston   \n",
      "2       Belkin 8 Outlet Surge Protector         Consumer    Louisville   \n",
      "3  GBC VeloBinder Manual Binding System        Corporate       Chicago   \n",
      "4         Eldon Pizzaz Desk Accessories      Home Office  Philadelphia   \n",
      "\n",
      "  customer_state customer_country customer_region  ...  order_date  \\\n",
      "0       New York    United States            East  ...   11/5/2022   \n",
      "1          Texas    United States         Central  ...  20-06-2022   \n",
      "2       Kentucky    United States           South  ...  25-06-2022   \n",
      "3       Illinois    United States         Central  ...   10/6/2022   \n",
      "4   Pennsylvania    United States            East  ...    2/5/2022   \n",
      "\n",
      "       order_id   ship_date   shipping_type days_for_shipment_scheduled  \\\n",
      "0  O_ID_3001072   11/7/2022    Second Class                           2   \n",
      "1  O_ID_3009170  23-06-2022    Second Class                           2   \n",
      "2  O_ID_3047567  30-06-2022  Standard Class                           4   \n",
      "3  O_ID_3060575  10/10/2022    Second Class                           2   \n",
      "4  O_ID_3064311    8/1/2022     First Class                           1   \n",
      "\n",
      "   days_for_shipment_real  order_item_discount  sales_per_order  \\\n",
      "0                       2                 35.0            500.0   \n",
      "1                       3                 85.0            500.0   \n",
      "2                       5                 75.0             44.0   \n",
      "3                       4                 60.0            254.0   \n",
      "4                       2                125.0            500.0   \n",
      "\n",
      "   order_quantity  profit_per_order  \n",
      "0               5        223.199997  \n",
      "1               5        199.199997  \n",
      "2               5        195.500000  \n",
      "3               1        220.000000  \n",
      "4               1         97.500000  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "DataFrame réduit :\n",
      "  customer_id                          product_name  order_date\n",
      "0  C_ID_45866                            Xerox 1913   11/5/2022\n",
      "1  C_ID_44932    #6 3/4 Gummed Flap White Envelopes  20-06-2022\n",
      "2  C_ID_70880       Belkin 8 Outlet Surge Protector  25-06-2022\n",
      "3  C_ID_33157  GBC VeloBinder Manual Binding System   10/6/2022\n",
      "4  C_ID_58303         Eldon Pizzaz Desk Accessories    2/5/2022\n",
      "Nombre de lignes après réduction : 22654\n",
      "       customer_id                                       product_name  \\\n",
      "112094  C_ID_25005                             GBC Wire Binding Combs   \n",
      "43163   C_ID_25006                     Cisco SPA525G2 5-Line IP Phone   \n",
      "43165   C_ID_25006  Kensington 7 Outlet MasterPiece Power Center w...   \n",
      "18488   C_ID_25008                                         Newell 318   \n",
      "29860   C_ID_25011      Wilson Jones 1\" Hanging DublLock Ring Binders   \n",
      "\n",
      "       order_date  \n",
      "112094 2022-08-24  \n",
      "43163  2021-07-24  \n",
      "43165  2021-08-20  \n",
      "18488  2022-02-15  \n",
      "29860  2022-10-22  \n",
      "  customer_id                                  products_sequence\n",
      "0  C_ID_25005                             GBC Wire Binding Combs\n",
      "1  C_ID_25006  Cisco SPA525G2 5-Line IP Phone, Kensington 7 O...\n",
      "2  C_ID_25008                                         Newell 318\n",
      "3  C_ID_25011      Wilson Jones 1\" Hanging DublLock Ring Binders\n",
      "4  C_ID_25012  Acme Office Executive Series Stainless Steel T...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cated\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 93ms/step - accuracy: 0.0161 - loss: 6.9091 - val_accuracy: 0.0496 - val_loss: 6.0094\n",
      "Epoch 2/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.0777 - loss: 5.6432 - val_accuracy: 0.1732 - val_loss: 4.8246\n",
      "Epoch 3/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 94ms/step - accuracy: 0.1998 - loss: 4.4608 - val_accuracy: 0.3230 - val_loss: 3.7451\n",
      "Epoch 4/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.3445 - loss: 3.4161 - val_accuracy: 0.4618 - val_loss: 2.9491\n",
      "Epoch 5/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 95ms/step - accuracy: 0.4539 - loss: 2.6736 - val_accuracy: 0.5489 - val_loss: 2.4398\n",
      "Epoch 6/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 107ms/step - accuracy: 0.5456 - loss: 2.1435 - val_accuracy: 0.6213 - val_loss: 2.0864\n",
      "Epoch 7/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 89ms/step - accuracy: 0.6069 - loss: 1.8002 - val_accuracy: 0.6630 - val_loss: 1.8558\n",
      "Epoch 8/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 105ms/step - accuracy: 0.6462 - loss: 1.5498 - val_accuracy: 0.6994 - val_loss: 1.6750\n",
      "Epoch 9/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 95ms/step - accuracy: 0.6883 - loss: 1.3607 - val_accuracy: 0.7184 - val_loss: 1.5800\n",
      "Epoch 10/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 92ms/step - accuracy: 0.7044 - loss: 1.2314 - val_accuracy: 0.7333 - val_loss: 1.4914\n",
      "Epoch 11/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 112ms/step - accuracy: 0.7234 - loss: 1.1309 - val_accuracy: 0.7465 - val_loss: 1.4345\n",
      "Epoch 12/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 98ms/step - accuracy: 0.7330 - loss: 1.0686 - val_accuracy: 0.7587 - val_loss: 1.3997\n",
      "Epoch 13/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 109ms/step - accuracy: 0.7449 - loss: 1.0058 - val_accuracy: 0.7627 - val_loss: 1.3794\n",
      "Epoch 14/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 120ms/step - accuracy: 0.7596 - loss: 0.9402 - val_accuracy: 0.7688 - val_loss: 1.3404\n",
      "Epoch 15/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 92ms/step - accuracy: 0.7687 - loss: 0.8891 - val_accuracy: 0.7706 - val_loss: 1.3339\n",
      "Epoch 16/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 99ms/step - accuracy: 0.7725 - loss: 0.8587 - val_accuracy: 0.7809 - val_loss: 1.3003\n",
      "Epoch 17/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 96ms/step - accuracy: 0.7796 - loss: 0.8341 - val_accuracy: 0.7823 - val_loss: 1.3071\n",
      "Epoch 18/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 101ms/step - accuracy: 0.7812 - loss: 0.8080 - val_accuracy: 0.7850 - val_loss: 1.2982\n",
      "Epoch 19/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 99ms/step - accuracy: 0.7812 - loss: 0.7956 - val_accuracy: 0.7808 - val_loss: 1.2923\n",
      "Epoch 20/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 95ms/step - accuracy: 0.7919 - loss: 0.7693 - val_accuracy: 0.7880 - val_loss: 1.2901\n",
      "Produits prédits : ['cubic', 'foot', 'mid']\n",
      "Suggestions d'améliorations pour les performances du modèle:\n",
      "- Augmenter la taille du dataset pour capturer plus de variations.\n",
      "- Utiliser des techniques de régularisation telles que la L2 regularization ou le dropout pour éviter le surapprentissage.\n",
      "- Expérimenter avec des architectures de modèles différentes, comme les GRU ou Transformer.\n",
      "- Fine-tuning des hyperparamètres tels que le taux d'apprentissage, la taille des couches, et le nombre de neurones.\n",
      "\n",
      "Métriques d'évaluation adaptées pour ce type de système de recommandation:\n",
      "- Précision@k : Pourcentage de fois où le produit correct est parmi les k premiers produits recommandés.\n",
      "- Recall@k : Pourcentage de fois où tous les produits corrects sont parmi les k premiers produits recommandés.\n",
      "- F1-Score : Harmonie entre la précision et le rappel pour évaluer le modèle.\n",
      "- AUC-ROC : Mesure la performance du modèle de classification à différents seuils de classification.\n",
      "- NDCG (Normalized Discounted Cumulative Gain) : Mesure l'efficacité des recommandations en prenant en compte la position des produits recommandés.\n"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, ndcg_score\n",
    "\n",
    "# Étape 2 : Charger et explorer les données\n",
    "df = pd.read_csv(r\"C:\\Users\\cated\\Desktop\\dev\\IA school\\Machine learning supervisé recommandation\\Ecommerce_data.csv\")\n",
    "print(\"Aperçu des colonnes :\")\n",
    "print(df.columns)\n",
    "print(\"\\nPremières lignes du dataset :\")\n",
    "print(df.head())\n",
    "\n",
    "# Étape 3 : Prétaitement des données\n",
    "df_reduit = df[['customer_id', 'product_name', 'order_date']]\n",
    "print(\"\\nDataFrame réduit :\")\n",
    "print(df_reduit.head())\n",
    "\n",
    "# Réduction de la taille du dataset à 20%\n",
    "df_reduit = df_reduit.sample(frac=0.20, random_state=1)\n",
    "print(f\"Nombre de lignes après réduction : {len(df_reduit)}\")\n",
    "\n",
    "# Conversion des dates et tri\n",
    "df_reduit['order_date'] = pd.to_datetime(df_reduit['order_date'], format=\"%d-%m-%Y\", errors='coerce')\n",
    "df_reduit['order_date'] = df_reduit['order_date'].fillna(pd.to_datetime(df_reduit['order_date'], format=\"%m/%d/%Y\", errors='coerce'))\n",
    "\n",
    "# Enlever les dates invalides\n",
    "df_reduit = df_reduit.dropna(subset=['order_date'])\n",
    "\n",
    "# Trier les achats par customer_id et order_date\n",
    "df_reduit = df_reduit.sort_values(by=['customer_id', 'order_date'])\n",
    "print(df_reduit.head())\n",
    "\n",
    "# Agrégation des séquences\n",
    "df_agg = df_reduit.groupby('customer_id')['product_name'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "df_agg.rename(columns={'product_name': 'products_sequence'}, inplace=True)\n",
    "print(df_agg.head())\n",
    "\n",
    "# Étape 4 : Tokenisation et création des paires d’entrée-sortie\n",
    "# 7. Tokenisation des séquences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_agg['products_sequence'])\n",
    "sequences = tokenizer.texts_to_sequences(df_agg['products_sequence'])\n",
    "\n",
    "# 8. Créer les paires entrée-sortie\n",
    "X, y = [], []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        X.append(seq[:i])\n",
    "        y.append(seq[i])\n",
    "\n",
    "# Trouver la longueur maximale des séquences\n",
    "max_sequence_length = max(len(seq) for seq in X)\n",
    "\n",
    "# 9. Remplir les séquences\n",
    "X = pad_sequences(X, maxlen=max_sequence_length, padding='pre')\n",
    "y = np.array(y)\n",
    "\n",
    "# Étape 5 : Création et entraînement du modèle LSTM\n",
    "# Définir les paramètres du modèle\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100  # Augmenter la dimension des embeddings\n",
    "max_sequence_length = max([len(x) for x in X])\n",
    "\n",
    "# Définir le modèle\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(X, y, epochs=20, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Étape 6 : Prédiction\n",
    "# 12. Créer une fonction de prédiction\n",
    "def predict_next_products(model, tokenizer, input_sequence, num_predictions=1):\n",
    "    \"\"\"\n",
    "    Prédire les prochains produits les plus probables.\n",
    "\n",
    "    Args:\n",
    "    model -- le modèle LSTM entraîné\n",
    "    tokenizer -- le tokenizer utilisé pour convertir les noms de produits\n",
    "    input_sequence -- la séquence d'entrée des produits\n",
    "    num_predictions -- le nombre de prédictions à générer\n",
    "\n",
    "    Retourne:\n",
    "    Une liste des prochains produits les plus probables\n",
    "    \"\"\"\n",
    "    for _ in range(num_predictions):\n",
    "        # Convertir la séquence d'entrée en indices\n",
    "        encoded_sequence = tokenizer.texts_to_sequences([input_sequence])[0]\n",
    "        # Ajouter un padding pour correspondre à la longueur maximale de séquence\n",
    "        padded_sequence = pad_sequences([encoded_sequence], maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "        # Prédire le produit suivant\n",
    "        predicted_index = model.predict(padded_sequence, verbose=0)\n",
    "        predicted_index = np.argmax(predicted_index, axis=1)[0]\n",
    "\n",
    "        # Convertir l'indice prédit en produit\n",
    "        predicted_product = tokenizer.index_word[predicted_index]\n",
    "        \n",
    "        # Ajouter le produit prédit à la séquence d'entrée\n",
    "        input_sequence += ' ' + predicted_product\n",
    "\n",
    "    return input_sequence.split()[-num_predictions:]\n",
    "\n",
    "# 13. Tester les prédictions\n",
    "example_sequence = \"product_1 product_2 product_3\"\n",
    "predicted_products = predict_next_products(model, tokenizer, example_sequence, num_predictions=3)\n",
    "print(f\"Produits prédits : {predicted_products}\")\n",
    "\n",
    "# Étape 7 : Questions de réflexion\n",
    "# 14. Améliorations possibles\n",
    "# Mettez en pratique les suggestions ci-dessous pour améliorer et évaluer le modèle\n",
    "\n",
    "# Quelles approches pourriez-vous utiliser pour améliorer les performances du modèle ?\n",
    "print(\"Suggestions d'améliorations pour les performances du modèle:\")\n",
    "print(\"- Augmenter la taille du dataset pour capturer plus de variations.\")\n",
    "print(\"- Utiliser des techniques de régularisation telles que la L2 regularization ou le dropout pour éviter le surapprentissage.\")\n",
    "print(\"- Expérimenter avec des architectures de modèles différentes, comme les GRU ou Transformer.\")\n",
    "print(\"- Fine-tuning des hyperparamètres tels que le taux d'apprentissage, la taille des couches, et le nombre de neurones.\")\n",
    "\n",
    "# Proposez des métriques d’évaluation adaptées pour ce type de système de recommandation\n",
    "print(\"\\nMétriques d'évaluation adaptées pour ce type de système de recommandation:\")\n",
    "print(\"- Précision@k : Pourcentage de fois où le produit correct est parmi les k premiers produits recommandés.\")\n",
    "print(\"- Recall@k : Pourcentage de fois où tous les produits corrects sont parmi les k premiers produits recommandés.\")\n",
    "print(\"- F1-Score : Harmonie entre la précision et le rappel pour évaluer le modèle.\")\n",
    "print(\"- AUC-ROC : Mesure la performance du modèle de classification à différents seuils de classification.\")\n",
    "print(\"- NDCG (Normalized Discounted Cumulative Gain) : Mesure l'efficacité des recommandations en prenant en compte la position des produits recommandés.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed64d5-64e4-47c4-ba32-c2063d11bb6f",
   "metadata": {},
   "source": [
    "# Rapport Technique : Système de Recommandation de Produits\n",
    "\n",
    "## Introduction\n",
    "Ce rapport décrit le processus de développement d'un système de recommandation de produits utilisant un modèle GRU (Gated Recurrent Unit). Le code est divisé en plusieurs étapes allant de l'importation des bibliothèques nécessaires à la création et l'évaluation du modèle. \n",
    "\n",
    "## Importation des bibliothèques\n",
    "Le code commence par l'importation des bibliothèques nécessaires pour la manipulation des données, la création du modèle et l'évaluation des performances.\n",
    "\n",
    "## Chargement et exploration des données\n",
    "Les données sont chargées à partir d'un fichier CSV. Les premières étapes consistent à afficher les colonnes du dataset ainsi que les premières lignes pour obtenir une vue d'ensemble des données.\n",
    "\n",
    "## Prétraitement des données\n",
    "Les données sont prétraitées pour ne conserver que les colonnes `customer_id`, `product_name` et `order_date`. Une réduction de la taille du dataset est effectuée en sélectionnant aléatoirement 20% des données. Les dates sont ensuite converties au format datetime et les enregistrements avec des dates invalides sont supprimés. Les données sont triées par `customer_id` et `order_date`, puis les séquences de produits par client sont agrégées.\n",
    "\n",
    "## Tokenisation et création des paires d'entrée-sortie\n",
    "Les séquences de produits sont tokenisées et transformées en indices numériques. Des paires d'entrée-sortie sont ensuite créées pour entraîner le modèle.\n",
    "\n",
    "## Création et entraînement du modèle GRU\n",
    "Le modèle GRU est défini avec une couche d'embedding, deux couches GRU, des couches de dropout pour éviter le surapprentissage et des couches denses pour la classification. Le modèle est compilé avec l'optimiseur Adam et la fonction de perte `sparse_categorical_crossentropy`, puis entraîné sur les données.\n",
    "\n",
    "## Prédiction\n",
    "Une fonction de prédiction est définie pour prédire les prochains produits les plus probables en utilisant le modèle entraîné. La fonction prend une séquence d'entrée et génère le nombre de prédictions souhaité.\n",
    "\n",
    "## Questions de réflexion et améliorations possibles\n",
    "Quelques suggestions pour améliorer les performances du modèle sont proposées, telles que l'augmentation de la taille du dataset, l'utilisation de techniques de régularisation, l'expérimentation avec différentes architectures de modèles et le réglage fin des hyperparamètres.\n",
    "\n",
    "Des métriques d'évaluation adaptées pour ce type de système de recommandation sont également proposées, notamment la Précision@k, le Recall@k, le F1-Score, l'AUC-ROC et le NDCG.\n",
    "\n",
    "## Conclusion\n",
    "Ce rapport présente une méthode de développement d'un système de recommandation de produits basé sur un modèle GRU. Les étapes vont du chargement et du prétraitement des données à la création, l'entraînement et l'évaluation du modèle. Des suggestions d'améliorations et des métriques d'évaluation sont également fournies pour optimiser les performances du système.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
