{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6870ae4d-22c2-4978-adb7-f6cca6e54384",
   "metadata": {},
   "source": [
    "# TP : SystÃ¨me de Recommandation avec un LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f09558-490c-40ea-92a1-cac30fe0c357",
   "metadata": {},
   "source": [
    "### Etape 1 : Importation des donnÃ©es\n",
    "\n",
    "#### 1. TÃ©lÃ©chargement des donnÃ©es :\n",
    "- TÃ©lÃ©chargez le dataset \"E-Commerce Dataset\" depuis Kaggle en utilisant lâ€™API kagglehub.\n",
    "- Lien vers le dataset : [E-Commerce Dataset](https://www.kaggle.com/datasets/davidafolayan/e-commerce-dataset/data)\n",
    "\n",
    "### Etape 2 : Chargement et exploration des donnÃ©es\n",
    "\n",
    "#### 2. Charger les donnÃ©es :\n",
    "- Chargez le fichier CSV contenant les donnÃ©es transactionnelles en utilisant pandas.\n",
    "- Affichez un aperÃ§u des colonnes disponibles et des premiÃ¨res lignes du dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75593d47-7cab-4c05-8d41-597af4d19c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_first_name</th>\n",
       "      <th>customer_last_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>product_name</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>customer_country</th>\n",
       "      <th>customer_region</th>\n",
       "      <th>...</th>\n",
       "      <th>order_date</th>\n",
       "      <th>order_id</th>\n",
       "      <th>ship_date</th>\n",
       "      <th>shipping_type</th>\n",
       "      <th>days_for_shipment_scheduled</th>\n",
       "      <th>days_for_shipment_real</th>\n",
       "      <th>order_item_discount</th>\n",
       "      <th>sales_per_order</th>\n",
       "      <th>order_quantity</th>\n",
       "      <th>profit_per_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_45866</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Fuller</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Xerox 1913</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>New Rochelle</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>East</td>\n",
       "      <td>...</td>\n",
       "      <td>11/5/2022</td>\n",
       "      <td>O_ID_3001072</td>\n",
       "      <td>11/7/2022</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>35.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5</td>\n",
       "      <td>223.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_44932</td>\n",
       "      <td>Alan</td>\n",
       "      <td>Edelman</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>#6 3/4 Gummed Flap White Envelopes</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Texas</td>\n",
       "      <td>United States</td>\n",
       "      <td>Central</td>\n",
       "      <td>...</td>\n",
       "      <td>20-06-2022</td>\n",
       "      <td>O_ID_3009170</td>\n",
       "      <td>23-06-2022</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5</td>\n",
       "      <td>199.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_70880</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Gayman</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Belkin 8 Outlet Surge Protector</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Louisville</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>United States</td>\n",
       "      <td>South</td>\n",
       "      <td>...</td>\n",
       "      <td>25-06-2022</td>\n",
       "      <td>O_ID_3047567</td>\n",
       "      <td>30-06-2022</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>75.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5</td>\n",
       "      <td>195.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_33157</td>\n",
       "      <td>Raymond</td>\n",
       "      <td>Eason</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>GBC VeloBinder Manual Binding System</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>United States</td>\n",
       "      <td>Central</td>\n",
       "      <td>...</td>\n",
       "      <td>10/6/2022</td>\n",
       "      <td>O_ID_3060575</td>\n",
       "      <td>10/10/2022</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>1</td>\n",
       "      <td>220.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_58303</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Gonzalez</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Eldon Pizzaz Desk Accessories</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>United States</td>\n",
       "      <td>East</td>\n",
       "      <td>...</td>\n",
       "      <td>2/5/2022</td>\n",
       "      <td>O_ID_3064311</td>\n",
       "      <td>8/1/2022</td>\n",
       "      <td>First Class</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>125.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>97.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id customer_first_name customer_last_name    category_name  \\\n",
       "0  C_ID_45866                Mary             Fuller  Office Supplies   \n",
       "1  C_ID_44932                Alan            Edelman  Office Supplies   \n",
       "2  C_ID_70880                Mary             Gayman  Office Supplies   \n",
       "3  C_ID_33157             Raymond              Eason  Office Supplies   \n",
       "4  C_ID_58303                Mary           Gonzalez        Furniture   \n",
       "\n",
       "                           product_name customer_segment customer_city  \\\n",
       "0                            Xerox 1913        Corporate  New Rochelle   \n",
       "1    #6 3/4 Gummed Flap White Envelopes        Corporate       Houston   \n",
       "2       Belkin 8 Outlet Surge Protector         Consumer    Louisville   \n",
       "3  GBC VeloBinder Manual Binding System        Corporate       Chicago   \n",
       "4         Eldon Pizzaz Desk Accessories      Home Office  Philadelphia   \n",
       "\n",
       "  customer_state customer_country customer_region  ...  order_date  \\\n",
       "0       New York    United States            East  ...   11/5/2022   \n",
       "1          Texas    United States         Central  ...  20-06-2022   \n",
       "2       Kentucky    United States           South  ...  25-06-2022   \n",
       "3       Illinois    United States         Central  ...   10/6/2022   \n",
       "4   Pennsylvania    United States            East  ...    2/5/2022   \n",
       "\n",
       "       order_id   ship_date   shipping_type days_for_shipment_scheduled  \\\n",
       "0  O_ID_3001072   11/7/2022    Second Class                           2   \n",
       "1  O_ID_3009170  23-06-2022    Second Class                           2   \n",
       "2  O_ID_3047567  30-06-2022  Standard Class                           4   \n",
       "3  O_ID_3060575  10/10/2022    Second Class                           2   \n",
       "4  O_ID_3064311    8/1/2022     First Class                           1   \n",
       "\n",
       "   days_for_shipment_real  order_item_discount  sales_per_order  \\\n",
       "0                       2                 35.0            500.0   \n",
       "1                       3                 85.0            500.0   \n",
       "2                       5                 75.0             44.0   \n",
       "3                       4                 60.0            254.0   \n",
       "4                       2                125.0            500.0   \n",
       "\n",
       "   order_quantity  profit_per_order  \n",
       "0               5        223.199997  \n",
       "1               5        199.199997  \n",
       "2               5        195.500000  \n",
       "3               1        220.000000  \n",
       "4               1         97.500000  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les donnÃ©es dans un DataFrame pandas\n",
    "df = pd.read_csv(r\"C:\\Users\\cated\\Desktop\\dev\\IA school\\Machine learning supervisÃ© recommandation\\Ecommerce_data.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e727c21-6c4e-47e3-8cfb-220fbb875629",
   "metadata": {},
   "source": [
    "### Etape 3 : PrÃ©taitement des donnÃ©es\n",
    "\n",
    "#### 3. SÃ©lectionner les colonnes pertinentes :\n",
    "- Gardez uniquement les colonnes suivantes : customer id, product name, et order date.\n",
    "\n",
    "#### 4. RÃ©duction de la taille du dataset :\n",
    "- RÃ©duisez la taille du dataset Ã  20 % pour simplifier lâ€™exÃ©cution des modÃ¨les.\n",
    "\n",
    "#### 5. Conversion des dates et tri :\n",
    "- Convertissez la colonne order date en format datetime et triez les achats par customer id et order date.\n",
    "\n",
    "#### 6. AgrÃ©gation des sÃ©quences :\n",
    "- Regroupez les achats par client en crÃ©ant une sÃ©quence unique de produits achetÃ©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b3c8bf8-1a15-4901-a3b1-2f558ddbbfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame rÃ©duit :\n",
      "  customer_id                          product_name  order_date\n",
      "0  C_ID_45866                            Xerox 1913   11/5/2022\n",
      "1  C_ID_44932    #6 3/4 Gummed Flap White Envelopes  20-06-2022\n",
      "2  C_ID_70880       Belkin 8 Outlet Surge Protector  25-06-2022\n",
      "3  C_ID_33157  GBC VeloBinder Manual Binding System   10/6/2022\n",
      "4  C_ID_58303         Eldon Pizzaz Desk Accessories    2/5/2022\n",
      "Nombre de lignes aprÃ¨s rÃ©duction : 22654\n",
      "       customer_id                                       product_name  \\\n",
      "112094  C_ID_25005                             GBC Wire Binding Combs   \n",
      "43163   C_ID_25006                     Cisco SPA525G2 5-Line IP Phone   \n",
      "43165   C_ID_25006  Kensington 7 Outlet MasterPiece Power Center w...   \n",
      "18488   C_ID_25008                                         Newell 318   \n",
      "29860   C_ID_25011      Wilson Jones 1\" Hanging DublLock Ring Binders   \n",
      "\n",
      "       order_date  \n",
      "112094 2022-08-24  \n",
      "43163  2021-07-24  \n",
      "43165  2021-08-20  \n",
      "18488  2022-02-15  \n",
      "29860  2022-10-22  \n",
      "  customer_id                                  products_sequence\n",
      "0  C_ID_25005                             GBC Wire Binding Combs\n",
      "1  C_ID_25006  Cisco SPA525G2 5-Line IP Phone, Kensington 7 O...\n",
      "2  C_ID_25008                                         Newell 318\n",
      "3  C_ID_25011      Wilson Jones 1\" Hanging DublLock Ring Binders\n",
      "4  C_ID_25012  Acme Office Executive Series Stainless Steel T...\n"
     ]
    }
   ],
   "source": [
    "# Ã‰tape 3 : PrÃ©taitement des donnÃ©es\n",
    "# SÃ©lectionner les colonnes pertinentes\n",
    "df_reduit = df[['customer_id', 'product_name', 'order_date']]\n",
    "\n",
    "# Afficher les premiÃ¨res lignes du DataFrame rÃ©duit\n",
    "print(\"\\nDataFrame rÃ©duit :\")\n",
    "print(df_reduit.head())\n",
    "\n",
    "# RÃ©duction de la taille du dataset Ã  20%\n",
    "df_reduit = df_reduit.sample(frac=0.20, random_state=1)\n",
    "print(f\"Nombre de lignes aprÃ¨s rÃ©duction : {len(df_reduit)}\")\n",
    "\n",
    "# Conversion des dates en format datetime en gÃ©rant les diffÃ©rents formats\n",
    "df_reduit['order_date'] = pd.to_datetime(df_reduit['order_date'], format=\"%d-%m-%Y\", errors='coerce')\n",
    "df_reduit['order_date'] = df_reduit['order_date'].fillna(pd.to_datetime(df_reduit['order_date'], format=\"%m/%d/%Y\", errors='coerce'))\n",
    "\n",
    "# Enlever les dates invalides\n",
    "df_reduit = df_reduit.dropna(subset=['order_date'])\n",
    "\n",
    "# Trier les achats par customer_id et order_date\n",
    "df_reduit = df_reduit.sort_values(by=['customer_id', 'order_date'])\n",
    "print(df_reduit.head())\n",
    "\n",
    "# AgrÃ©gation des sÃ©quences\n",
    "df_agg = df_reduit.groupby('customer_id')['product_name'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "df_agg.rename(columns={'product_name': 'products_sequence'}, inplace=True)\n",
    "print(df_agg.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d525d-e0ad-4818-a4fb-f3df392aea10",
   "metadata": {},
   "source": [
    "### Etape 4 : Tokenisation et crÃ©ation des paires dâ€™entrÃ©e-sortie\n",
    "\n",
    "#### 7. Tokenisation des sÃ©quences :\n",
    "- Utilisez un tokenizer pour convertir les noms de produits en indices numÃ©riques.\n",
    "\n",
    "#### 8. CrÃ©er les paires entrÃ©e-sortie :\n",
    "- CrÃ©ez des paires X et y oÃ¹ X est une sÃ©quence partielle et y est le produit suivant dans la sÃ©quence.\n",
    "\n",
    "#### 9. Remplir les sÃ©quences :\n",
    "- Utilisez le padding pour uniformiser les longueurs des sÃ©quences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a8021489-63ba-45d4-8d39-320a4110730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# 7. Tokenisation des sÃ©quences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_agg['products_sequence'])\n",
    "sequences = tokenizer.texts_to_sequences(df_agg['products_sequence'])\n",
    "\n",
    "# 8. CrÃ©er les paires entrÃ©e-sortie\n",
    "X, y = [], []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        X.append(seq[:i])\n",
    "        y.append(seq[i])\n",
    "\n",
    "# Trouver la longueur maximale des sÃ©quences\n",
    "max_sequence_length = max(len(seq) for seq in X)\n",
    "\n",
    "# 9. Remplir les sÃ©quences\n",
    "X = pad_sequences(X, maxlen=max_sequence_length, padding='pre')\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931a437-da3c-48cb-9595-930330fcf2ee",
   "metadata": {},
   "source": [
    "### Etape 5 : CrÃ©ation et entraÃ®nement du modÃ¨le LSTM\n",
    "\n",
    "#### 10. DÃ©finir le modÃ¨le :\n",
    "- Construisez un modÃ¨le LSTM avec les couches suivantes :\n",
    "  - Une couche Embedding pour convertir les indices des produits en vecteurs de caractÃ©ristiques.\n",
    "  - Une couche LSTM.\n",
    "  - Deux couches Dense, dont la derniÃ¨re avec une activation softmax.\n",
    "\n",
    "### 11. Compiler et entraÃ®ner le modÃ¨le :\n",
    "- Compilez le modÃ¨le avec une fonction de perte adaptÃ©e et entraÃ®nez-le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e1652a2-cbc1-4cc1-bf39-4c1c377e325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cated\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - accuracy: 6.2500e-04 - loss: 5.9992 - val_accuracy: 0.0099 - val_loss: 6.0025\n",
      "Epoch 2/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0072 - loss: 5.9895 - val_accuracy: 0.0099 - val_loss: 6.0208\n",
      "Epoch 3/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0132 - loss: 5.9410 - val_accuracy: 0.0000e+00 - val_loss: 6.2588\n",
      "Epoch 4/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0135 - loss: 5.7908 - val_accuracy: 0.0000e+00 - val_loss: 6.5318\n",
      "Epoch 5/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.0122 - loss: 5.6875 - val_accuracy: 0.0000e+00 - val_loss: 6.7070\n",
      "Epoch 6/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.0174 - loss: 5.6641 - val_accuracy: 0.0000e+00 - val_loss: 6.8708\n",
      "Epoch 7/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.0097 - loss: 5.6022 - val_accuracy: 0.0000e+00 - val_loss: 7.0638\n",
      "Epoch 8/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.0093 - loss: 5.5744 - val_accuracy: 0.0000e+00 - val_loss: 7.1794\n",
      "Epoch 9/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.0156 - loss: 5.5758 - val_accuracy: 0.0000e+00 - val_loss: 7.2941\n",
      "Epoch 10/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0132 - loss: 5.5620 - val_accuracy: 0.0000e+00 - val_loss: 7.3603\n",
      "Epoch 11/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0221 - loss: 5.5323 - val_accuracy: 0.0000e+00 - val_loss: 7.4189\n",
      "Epoch 12/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0124 - loss: 5.5529 - val_accuracy: 0.0000e+00 - val_loss: 7.4319\n",
      "Epoch 13/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0136 - loss: 5.5754 - val_accuracy: 0.0000e+00 - val_loss: 7.5531\n",
      "Epoch 14/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.0166 - loss: 5.5130 - val_accuracy: 0.0099 - val_loss: 7.7684\n",
      "Epoch 15/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0236 - loss: 5.5097 - val_accuracy: 0.0099 - val_loss: 7.7681\n",
      "Epoch 16/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0082 - loss: 5.5227 - val_accuracy: 0.0099 - val_loss: 7.6152\n",
      "Epoch 17/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0169 - loss: 5.5089 - val_accuracy: 0.0099 - val_loss: 7.6272\n",
      "Epoch 18/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0243 - loss: 5.4558 - val_accuracy: 0.0099 - val_loss: 7.7480\n",
      "Epoch 19/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.0171 - loss: 5.4166 - val_accuracy: 0.0099 - val_loss: 7.7867\n",
      "Epoch 20/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.0228 - loss: 5.4295 - val_accuracy: 0.0099 - val_loss: 7.8078\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# DÃ©finir les paramÃ¨tres du modÃ¨le\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Taille du vocabulaire\n",
    "embedding_dim = 50  # Dimension des embeddings\n",
    "max_sequence_length = max([len(x) for x in X])  # Longueur maximale des sÃ©quences\n",
    "\n",
    "# 10. DÃ©finir le modÃ¨le\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# 11. Compiler et entraÃ®ner le modÃ¨le\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# EntraÃ®ner le modÃ¨le\n",
    "history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59853f8-f83e-4d7d-bc35-337551541c1f",
   "metadata": {},
   "source": [
    "### Etape 6 : PrÃ©diction\n",
    "\n",
    "#### 12. CrÃ©er une fonction de prÃ©diction :\n",
    "- ImplÃ©mentez une fonction qui prend une sÃ©quence de produits et retourne les prochains produits les plus probables.\n",
    "\n",
    "#### 13. Tester les prÃ©dictions :\n",
    "- Utilisez un exemple de sÃ©quence pour gÃ©nÃ©rer des prÃ©dictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72040bec-e672-45ba-8f78-929f01866628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. CrÃ©er une fonction de prÃ©diction\n",
    "def predict_next_products(model, tokenizer, input_sequence, num_predictions=1):\n",
    "    \"\"\"\n",
    "    PrÃ©dire les prochains produits les plus probables.\n",
    "\n",
    "    Args:\n",
    "    model -- le modÃ¨le LSTM entraÃ®nÃ©\n",
    "    tokenizer -- le tokenizer utilisÃ© pour convertir les noms de produits\n",
    "    input_sequence -- la sÃ©quence d'entrÃ©e des produits\n",
    "    num_predictions -- le nombre de prÃ©dictions Ã  gÃ©nÃ©rer\n",
    "\n",
    "    Retourne:\n",
    "    Une liste des prochains produits les plus probables\n",
    "    \"\"\"\n",
    "    for _ in range(num_predictions):\n",
    "        # Convertir la sÃ©quence d'entrÃ©e en indices\n",
    "        encoded_sequence = tokenizer.texts_to_sequences([input_sequence])[0]\n",
    "        # Ajouter un padding pour correspondre Ã  la longueur maximale de sÃ©quence\n",
    "        padded_sequence = pad_sequences([encoded_sequence], maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "        # PrÃ©dire le produit suivant\n",
    "        predicted_index = model.predict(padded_sequence, verbose=0)\n",
    "        predicted_index = np.argmax(predicted_index, axis=1)[0]\n",
    "\n",
    "        # Convertir l'indice prÃ©dit en produit\n",
    "        predicted_product = tokenizer.index_word[predicted_index]\n",
    "        \n",
    "        # Ajouter le produit prÃ©dit Ã  la sÃ©quence d'entrÃ©e\n",
    "        input_sequence += ' ' + predicted_product\n",
    "\n",
    "    return input_sequence.split()[-num_predictions:]\n",
    "\n",
    "# 13. Tester les prÃ©dictions\n",
    "example_sequence = \"product_1 product_2 product_3\"\n",
    "predicted_products = predict_next_products(model, tokenizer, example_sequence, num_predictions=3)\n",
    "print(f\"Produits prÃ©dits : {predicted_products}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57320c-a39c-4e12-89d3-d82e8a81d31d",
   "metadata": {},
   "source": [
    "### Etape 7 : Questions de rÃ©flexion\n",
    "\n",
    "#### 14. AmÃ©liorations possibles :\n",
    "- Quelles approches pourriez-vous utiliser pour amÃ©liorer les performances du modÃ¨le ?\n",
    "  - Augmenter la taille du dataset pour capturer plus de variations.\n",
    "  - Utiliser des techniques de rÃ©gularisation telles que la L2 regularization ou le dropout pour Ã©viter le surapprentissage.\n",
    "  - ExpÃ©rimenter avec des architectures de modÃ¨les diffÃ©rentes, comme les GRU ou Transformer.\n",
    "  - Fine-tuning des hyperparamÃ¨tres tels que le taux d'apprentissage, la taille des couches, et le nombre de neurones.\n",
    "\n",
    "- Proposez des mÃ©triques dâ€™Ã©valuation adaptÃ©es pour ce type de systÃ¨me de recommandation.\n",
    "  - PrÃ©cision@k : Pourcentage de fois oÃ¹ le produit correct est parmi les k premiers produits recommandÃ©s.\n",
    "  - Recall@k : Pourcentage de fois oÃ¹ tous les produits corrects sont parmi les k premiers produits recommandÃ©s.\n",
    "  - F1-Score : Harmonie entre la prÃ©cision et le rappel pour Ã©valuer le modÃ¨le.\n",
    "  - AUC-ROC : Mesure la performance du modÃ¨le de classification Ã  diffÃ©rents seuils de classification.\n",
    "  - NDCG (Normalized Discounted Cumulative Gain) : Mesure l'efficacitÃ© des recommandations en prenant en compte la position des produits recommandÃ©s.\n",
    "\n",
    "Essayez ce code pour implÃ©menter les prÃ©dictions et utilisez les questions de rÃ©flexion pour amÃ©liorer et Ã©valuer votre modÃ¨le. Si vous avez besoin d'aide supplÃ©mentaire ou d'autres modifications, n'hÃ©sitez pas Ã  demander! ğŸ˜Š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d298f905-95f4-4135-8c2b-7b854a808354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AperÃ§u des colonnes :\n",
      "Index(['customer_id', 'customer_first_name', 'customer_last_name',\n",
      "       'category_name', 'product_name', 'customer_segment', 'customer_city',\n",
      "       'customer_state', 'customer_country', 'customer_region',\n",
      "       'delivery_status', 'order_date', 'order_id', 'ship_date',\n",
      "       'shipping_type', 'days_for_shipment_scheduled',\n",
      "       'days_for_shipment_real', 'order_item_discount', 'sales_per_order',\n",
      "       'order_quantity', 'profit_per_order'],\n",
      "      dtype='object')\n",
      "\n",
      "PremiÃ¨res lignes du dataset :\n",
      "  customer_id customer_first_name customer_last_name    category_name  \\\n",
      "0  C_ID_45866                Mary             Fuller  Office Supplies   \n",
      "1  C_ID_44932                Alan            Edelman  Office Supplies   \n",
      "2  C_ID_70880                Mary             Gayman  Office Supplies   \n",
      "3  C_ID_33157             Raymond              Eason  Office Supplies   \n",
      "4  C_ID_58303                Mary           Gonzalez        Furniture   \n",
      "\n",
      "                           product_name customer_segment customer_city  \\\n",
      "0                            Xerox 1913        Corporate  New Rochelle   \n",
      "1    #6 3/4 Gummed Flap White Envelopes        Corporate       Houston   \n",
      "2       Belkin 8 Outlet Surge Protector         Consumer    Louisville   \n",
      "3  GBC VeloBinder Manual Binding System        Corporate       Chicago   \n",
      "4         Eldon Pizzaz Desk Accessories      Home Office  Philadelphia   \n",
      "\n",
      "  customer_state customer_country customer_region  ...  order_date  \\\n",
      "0       New York    United States            East  ...   11/5/2022   \n",
      "1          Texas    United States         Central  ...  20-06-2022   \n",
      "2       Kentucky    United States           South  ...  25-06-2022   \n",
      "3       Illinois    United States         Central  ...   10/6/2022   \n",
      "4   Pennsylvania    United States            East  ...    2/5/2022   \n",
      "\n",
      "       order_id   ship_date   shipping_type days_for_shipment_scheduled  \\\n",
      "0  O_ID_3001072   11/7/2022    Second Class                           2   \n",
      "1  O_ID_3009170  23-06-2022    Second Class                           2   \n",
      "2  O_ID_3047567  30-06-2022  Standard Class                           4   \n",
      "3  O_ID_3060575  10/10/2022    Second Class                           2   \n",
      "4  O_ID_3064311    8/1/2022     First Class                           1   \n",
      "\n",
      "   days_for_shipment_real  order_item_discount  sales_per_order  \\\n",
      "0                       2                 35.0            500.0   \n",
      "1                       3                 85.0            500.0   \n",
      "2                       5                 75.0             44.0   \n",
      "3                       4                 60.0            254.0   \n",
      "4                       2                125.0            500.0   \n",
      "\n",
      "   order_quantity  profit_per_order  \n",
      "0               5        223.199997  \n",
      "1               5        199.199997  \n",
      "2               5        195.500000  \n",
      "3               1        220.000000  \n",
      "4               1         97.500000  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "DataFrame rÃ©duit :\n",
      "  customer_id                          product_name  order_date\n",
      "0  C_ID_45866                            Xerox 1913   11/5/2022\n",
      "1  C_ID_44932    #6 3/4 Gummed Flap White Envelopes  20-06-2022\n",
      "2  C_ID_70880       Belkin 8 Outlet Surge Protector  25-06-2022\n",
      "3  C_ID_33157  GBC VeloBinder Manual Binding System   10/6/2022\n",
      "4  C_ID_58303         Eldon Pizzaz Desk Accessories    2/5/2022\n",
      "Nombre de lignes aprÃ¨s rÃ©duction : 22654\n",
      "       customer_id                                       product_name  \\\n",
      "112094  C_ID_25005                             GBC Wire Binding Combs   \n",
      "43163   C_ID_25006                     Cisco SPA525G2 5-Line IP Phone   \n",
      "43165   C_ID_25006  Kensington 7 Outlet MasterPiece Power Center w...   \n",
      "18488   C_ID_25008                                         Newell 318   \n",
      "29860   C_ID_25011      Wilson Jones 1\" Hanging DublLock Ring Binders   \n",
      "\n",
      "       order_date  \n",
      "112094 2022-08-24  \n",
      "43163  2021-07-24  \n",
      "43165  2021-08-20  \n",
      "18488  2022-02-15  \n",
      "29860  2022-10-22  \n",
      "  customer_id                                  products_sequence\n",
      "0  C_ID_25005                             GBC Wire Binding Combs\n",
      "1  C_ID_25006  Cisco SPA525G2 5-Line IP Phone, Kensington 7 O...\n",
      "2  C_ID_25008                                         Newell 318\n",
      "3  C_ID_25011      Wilson Jones 1\" Hanging DublLock Ring Binders\n",
      "4  C_ID_25012  Acme Office Executive Series Stainless Steel T...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cated\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 93ms/step - accuracy: 0.0161 - loss: 6.9091 - val_accuracy: 0.0496 - val_loss: 6.0094\n",
      "Epoch 2/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.0777 - loss: 5.6432 - val_accuracy: 0.1732 - val_loss: 4.8246\n",
      "Epoch 3/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 94ms/step - accuracy: 0.1998 - loss: 4.4608 - val_accuracy: 0.3230 - val_loss: 3.7451\n",
      "Epoch 4/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 98ms/step - accuracy: 0.3445 - loss: 3.4161 - val_accuracy: 0.4618 - val_loss: 2.9491\n",
      "Epoch 5/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 95ms/step - accuracy: 0.4539 - loss: 2.6736 - val_accuracy: 0.5489 - val_loss: 2.4398\n",
      "Epoch 6/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 107ms/step - accuracy: 0.5456 - loss: 2.1435 - val_accuracy: 0.6213 - val_loss: 2.0864\n",
      "Epoch 7/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 89ms/step - accuracy: 0.6069 - loss: 1.8002 - val_accuracy: 0.6630 - val_loss: 1.8558\n",
      "Epoch 8/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 105ms/step - accuracy: 0.6462 - loss: 1.5498 - val_accuracy: 0.6994 - val_loss: 1.6750\n",
      "Epoch 9/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 95ms/step - accuracy: 0.6883 - loss: 1.3607 - val_accuracy: 0.7184 - val_loss: 1.5800\n",
      "Epoch 10/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 92ms/step - accuracy: 0.7044 - loss: 1.2314 - val_accuracy: 0.7333 - val_loss: 1.4914\n",
      "Epoch 11/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 112ms/step - accuracy: 0.7234 - loss: 1.1309 - val_accuracy: 0.7465 - val_loss: 1.4345\n",
      "Epoch 12/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 98ms/step - accuracy: 0.7330 - loss: 1.0686 - val_accuracy: 0.7587 - val_loss: 1.3997\n",
      "Epoch 13/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 109ms/step - accuracy: 0.7449 - loss: 1.0058 - val_accuracy: 0.7627 - val_loss: 1.3794\n",
      "Epoch 14/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 120ms/step - accuracy: 0.7596 - loss: 0.9402 - val_accuracy: 0.7688 - val_loss: 1.3404\n",
      "Epoch 15/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 92ms/step - accuracy: 0.7687 - loss: 0.8891 - val_accuracy: 0.7706 - val_loss: 1.3339\n",
      "Epoch 16/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 99ms/step - accuracy: 0.7725 - loss: 0.8587 - val_accuracy: 0.7809 - val_loss: 1.3003\n",
      "Epoch 17/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 96ms/step - accuracy: 0.7796 - loss: 0.8341 - val_accuracy: 0.7823 - val_loss: 1.3071\n",
      "Epoch 18/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 101ms/step - accuracy: 0.7812 - loss: 0.8080 - val_accuracy: 0.7850 - val_loss: 1.2982\n",
      "Epoch 19/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 99ms/step - accuracy: 0.7812 - loss: 0.7956 - val_accuracy: 0.7808 - val_loss: 1.2923\n",
      "Epoch 20/20\n",
      "\u001b[1m847/847\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 95ms/step - accuracy: 0.7919 - loss: 0.7693 - val_accuracy: 0.7880 - val_loss: 1.2901\n",
      "Produits prÃ©dits : ['cubic', 'foot', 'mid']\n",
      "Suggestions d'amÃ©liorations pour les performances du modÃ¨le:\n",
      "- Augmenter la taille du dataset pour capturer plus de variations.\n",
      "- Utiliser des techniques de rÃ©gularisation telles que la L2 regularization ou le dropout pour Ã©viter le surapprentissage.\n",
      "- ExpÃ©rimenter avec des architectures de modÃ¨les diffÃ©rentes, comme les GRU ou Transformer.\n",
      "- Fine-tuning des hyperparamÃ¨tres tels que le taux d'apprentissage, la taille des couches, et le nombre de neurones.\n",
      "\n",
      "MÃ©triques d'Ã©valuation adaptÃ©es pour ce type de systÃ¨me de recommandation:\n",
      "- PrÃ©cision@k : Pourcentage de fois oÃ¹ le produit correct est parmi les k premiers produits recommandÃ©s.\n",
      "- Recall@k : Pourcentage de fois oÃ¹ tous les produits corrects sont parmi les k premiers produits recommandÃ©s.\n",
      "- F1-Score : Harmonie entre la prÃ©cision et le rappel pour Ã©valuer le modÃ¨le.\n",
      "- AUC-ROC : Mesure la performance du modÃ¨le de classification Ã  diffÃ©rents seuils de classification.\n",
      "- NDCG (Normalized Discounted Cumulative Gain) : Mesure l'efficacitÃ© des recommandations en prenant en compte la position des produits recommandÃ©s.\n"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothÃ¨ques nÃ©cessaires\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, ndcg_score\n",
    "\n",
    "# Ã‰tape 2 : Charger et explorer les donnÃ©es\n",
    "df = pd.read_csv(r\"C:\\Users\\cated\\Desktop\\dev\\IA school\\Machine learning supervisÃ© recommandation\\Ecommerce_data.csv\")\n",
    "print(\"AperÃ§u des colonnes :\")\n",
    "print(df.columns)\n",
    "print(\"\\nPremiÃ¨res lignes du dataset :\")\n",
    "print(df.head())\n",
    "\n",
    "# Ã‰tape 3 : PrÃ©taitement des donnÃ©es\n",
    "df_reduit = df[['customer_id', 'product_name', 'order_date']]\n",
    "print(\"\\nDataFrame rÃ©duit :\")\n",
    "print(df_reduit.head())\n",
    "\n",
    "# RÃ©duction de la taille du dataset Ã  20%\n",
    "df_reduit = df_reduit.sample(frac=0.20, random_state=1)\n",
    "print(f\"Nombre de lignes aprÃ¨s rÃ©duction : {len(df_reduit)}\")\n",
    "\n",
    "# Conversion des dates et tri\n",
    "df_reduit['order_date'] = pd.to_datetime(df_reduit['order_date'], format=\"%d-%m-%Y\", errors='coerce')\n",
    "df_reduit['order_date'] = df_reduit['order_date'].fillna(pd.to_datetime(df_reduit['order_date'], format=\"%m/%d/%Y\", errors='coerce'))\n",
    "\n",
    "# Enlever les dates invalides\n",
    "df_reduit = df_reduit.dropna(subset=['order_date'])\n",
    "\n",
    "# Trier les achats par customer_id et order_date\n",
    "df_reduit = df_reduit.sort_values(by=['customer_id', 'order_date'])\n",
    "print(df_reduit.head())\n",
    "\n",
    "# AgrÃ©gation des sÃ©quences\n",
    "df_agg = df_reduit.groupby('customer_id')['product_name'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "df_agg.rename(columns={'product_name': 'products_sequence'}, inplace=True)\n",
    "print(df_agg.head())\n",
    "\n",
    "# Ã‰tape 4 : Tokenisation et crÃ©ation des paires dâ€™entrÃ©e-sortie\n",
    "# 7. Tokenisation des sÃ©quences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_agg['products_sequence'])\n",
    "sequences = tokenizer.texts_to_sequences(df_agg['products_sequence'])\n",
    "\n",
    "# 8. CrÃ©er les paires entrÃ©e-sortie\n",
    "X, y = [], []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        X.append(seq[:i])\n",
    "        y.append(seq[i])\n",
    "\n",
    "# Trouver la longueur maximale des sÃ©quences\n",
    "max_sequence_length = max(len(seq) for seq in X)\n",
    "\n",
    "# 9. Remplir les sÃ©quences\n",
    "X = pad_sequences(X, maxlen=max_sequence_length, padding='pre')\n",
    "y = np.array(y)\n",
    "\n",
    "# Ã‰tape 5 : CrÃ©ation et entraÃ®nement du modÃ¨le LSTM\n",
    "# DÃ©finir les paramÃ¨tres du modÃ¨le\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100  # Augmenter la dimension des embeddings\n",
    "max_sequence_length = max([len(x) for x in X])\n",
    "\n",
    "# DÃ©finir le modÃ¨le\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compiler le modÃ¨le\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# EntraÃ®ner le modÃ¨le\n",
    "history = model.fit(X, y, epochs=20, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Ã‰tape 6 : PrÃ©diction\n",
    "# 12. CrÃ©er une fonction de prÃ©diction\n",
    "def predict_next_products(model, tokenizer, input_sequence, num_predictions=1):\n",
    "    \"\"\"\n",
    "    PrÃ©dire les prochains produits les plus probables.\n",
    "\n",
    "    Args:\n",
    "    model -- le modÃ¨le LSTM entraÃ®nÃ©\n",
    "    tokenizer -- le tokenizer utilisÃ© pour convertir les noms de produits\n",
    "    input_sequence -- la sÃ©quence d'entrÃ©e des produits\n",
    "    num_predictions -- le nombre de prÃ©dictions Ã  gÃ©nÃ©rer\n",
    "\n",
    "    Retourne:\n",
    "    Une liste des prochains produits les plus probables\n",
    "    \"\"\"\n",
    "    for _ in range(num_predictions):\n",
    "        # Convertir la sÃ©quence d'entrÃ©e en indices\n",
    "        encoded_sequence = tokenizer.texts_to_sequences([input_sequence])[0]\n",
    "        # Ajouter un padding pour correspondre Ã  la longueur maximale de sÃ©quence\n",
    "        padded_sequence = pad_sequences([encoded_sequence], maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "        # PrÃ©dire le produit suivant\n",
    "        predicted_index = model.predict(padded_sequence, verbose=0)\n",
    "        predicted_index = np.argmax(predicted_index, axis=1)[0]\n",
    "\n",
    "        # Convertir l'indice prÃ©dit en produit\n",
    "        predicted_product = tokenizer.index_word[predicted_index]\n",
    "        \n",
    "        # Ajouter le produit prÃ©dit Ã  la sÃ©quence d'entrÃ©e\n",
    "        input_sequence += ' ' + predicted_product\n",
    "\n",
    "    return input_sequence.split()[-num_predictions:]\n",
    "\n",
    "# 13. Tester les prÃ©dictions\n",
    "example_sequence = \"product_1 product_2 product_3\"\n",
    "predicted_products = predict_next_products(model, tokenizer, example_sequence, num_predictions=3)\n",
    "print(f\"Produits prÃ©dits : {predicted_products}\")\n",
    "\n",
    "# Ã‰tape 7 : Questions de rÃ©flexion\n",
    "# 14. AmÃ©liorations possibles\n",
    "# Mettez en pratique les suggestions ci-dessous pour amÃ©liorer et Ã©valuer le modÃ¨le\n",
    "\n",
    "# Quelles approches pourriez-vous utiliser pour amÃ©liorer les performances du modÃ¨le ?\n",
    "print(\"Suggestions d'amÃ©liorations pour les performances du modÃ¨le:\")\n",
    "print(\"- Augmenter la taille du dataset pour capturer plus de variations.\")\n",
    "print(\"- Utiliser des techniques de rÃ©gularisation telles que la L2 regularization ou le dropout pour Ã©viter le surapprentissage.\")\n",
    "print(\"- ExpÃ©rimenter avec des architectures de modÃ¨les diffÃ©rentes, comme les GRU ou Transformer.\")\n",
    "print(\"- Fine-tuning des hyperparamÃ¨tres tels que le taux d'apprentissage, la taille des couches, et le nombre de neurones.\")\n",
    "\n",
    "# Proposez des mÃ©triques dâ€™Ã©valuation adaptÃ©es pour ce type de systÃ¨me de recommandation\n",
    "print(\"\\nMÃ©triques d'Ã©valuation adaptÃ©es pour ce type de systÃ¨me de recommandation:\")\n",
    "print(\"- PrÃ©cision@k : Pourcentage de fois oÃ¹ le produit correct est parmi les k premiers produits recommandÃ©s.\")\n",
    "print(\"- Recall@k : Pourcentage de fois oÃ¹ tous les produits corrects sont parmi les k premiers produits recommandÃ©s.\")\n",
    "print(\"- F1-Score : Harmonie entre la prÃ©cision et le rappel pour Ã©valuer le modÃ¨le.\")\n",
    "print(\"- AUC-ROC : Mesure la performance du modÃ¨le de classification Ã  diffÃ©rents seuils de classification.\")\n",
    "print(\"- NDCG (Normalized Discounted Cumulative Gain) : Mesure l'efficacitÃ© des recommandations en prenant en compte la position des produits recommandÃ©s.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed64d5-64e4-47c4-ba32-c2063d11bb6f",
   "metadata": {},
   "source": [
    "# Rapport Technique : SystÃ¨me de Recommandation de Produits\n",
    "\n",
    "## Introduction\n",
    "Ce rapport dÃ©crit le processus de dÃ©veloppement d'un systÃ¨me de recommandation de produits utilisant un modÃ¨le GRU (Gated Recurrent Unit). Le code est divisÃ© en plusieurs Ã©tapes allant de l'importation des bibliothÃ¨ques nÃ©cessaires Ã  la crÃ©ation et l'Ã©valuation du modÃ¨le. \n",
    "\n",
    "## Importation des bibliothÃ¨ques\n",
    "Le code commence par l'importation des bibliothÃ¨ques nÃ©cessaires pour la manipulation des donnÃ©es, la crÃ©ation du modÃ¨le et l'Ã©valuation des performances.\n",
    "\n",
    "## Chargement et exploration des donnÃ©es\n",
    "Les donnÃ©es sont chargÃ©es Ã  partir d'un fichier CSV. Les premiÃ¨res Ã©tapes consistent Ã  afficher les colonnes du dataset ainsi que les premiÃ¨res lignes pour obtenir une vue d'ensemble des donnÃ©es.\n",
    "\n",
    "## PrÃ©traitement des donnÃ©es\n",
    "Les donnÃ©es sont prÃ©traitÃ©es pour ne conserver que les colonnes `customer_id`, `product_name` et `order_date`. Une rÃ©duction de la taille du dataset est effectuÃ©e en sÃ©lectionnant alÃ©atoirement 20% des donnÃ©es. Les dates sont ensuite converties au format datetime et les enregistrements avec des dates invalides sont supprimÃ©s. Les donnÃ©es sont triÃ©es par `customer_id` et `order_date`, puis les sÃ©quences de produits par client sont agrÃ©gÃ©es.\n",
    "\n",
    "## Tokenisation et crÃ©ation des paires d'entrÃ©e-sortie\n",
    "Les sÃ©quences de produits sont tokenisÃ©es et transformÃ©es en indices numÃ©riques. Des paires d'entrÃ©e-sortie sont ensuite crÃ©Ã©es pour entraÃ®ner le modÃ¨le.\n",
    "\n",
    "## CrÃ©ation et entraÃ®nement du modÃ¨le GRU\n",
    "Le modÃ¨le GRU est dÃ©fini avec une couche d'embedding, deux couches GRU, des couches de dropout pour Ã©viter le surapprentissage et des couches denses pour la classification. Le modÃ¨le est compilÃ© avec l'optimiseur Adam et la fonction de perte `sparse_categorical_crossentropy`, puis entraÃ®nÃ© sur les donnÃ©es.\n",
    "\n",
    "## PrÃ©diction\n",
    "Une fonction de prÃ©diction est dÃ©finie pour prÃ©dire les prochains produits les plus probables en utilisant le modÃ¨le entraÃ®nÃ©. La fonction prend une sÃ©quence d'entrÃ©e et gÃ©nÃ¨re le nombre de prÃ©dictions souhaitÃ©.\n",
    "\n",
    "## Questions de rÃ©flexion et amÃ©liorations possibles\n",
    "Quelques suggestions pour amÃ©liorer les performances du modÃ¨le sont proposÃ©es, telles que l'augmentation de la taille du dataset, l'utilisation de techniques de rÃ©gularisation, l'expÃ©rimentation avec diffÃ©rentes architectures de modÃ¨les et le rÃ©glage fin des hyperparamÃ¨tres.\n",
    "\n",
    "Des mÃ©triques d'Ã©valuation adaptÃ©es pour ce type de systÃ¨me de recommandation sont Ã©galement proposÃ©es, notamment la PrÃ©cision@k, le Recall@k, le F1-Score, l'AUC-ROC et le NDCG.\n",
    "\n",
    "## Conclusion\n",
    "Ce rapport prÃ©sente une mÃ©thode de dÃ©veloppement d'un systÃ¨me de recommandation de produits basÃ© sur un modÃ¨le GRU. Les Ã©tapes vont du chargement et du prÃ©traitement des donnÃ©es Ã  la crÃ©ation, l'entraÃ®nement et l'Ã©valuation du modÃ¨le. Des suggestions d'amÃ©liorations et des mÃ©triques d'Ã©valuation sont Ã©galement fournies pour optimiser les performances du systÃ¨me.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
